{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Special Topics: Natural Language Processing\n",
    "## Assignment #2: Word2Vec Implementation\n",
    "## Clayton Haley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from math import log2\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import optimizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import string\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "import spacy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1395</td>\n",
       "      <td>1396</td>\n",
       "      <td>B00068PCTU</td>\n",
       "      <td>A1WLOJBYGZ49HL</td>\n",
       "      <td>Brian F., Gloucester, MA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1333843200</td>\n",
       "      <td>The best I've ever had!</td>\n",
       "      <td>I am so happy I discovered this product. I had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>343222</td>\n",
       "      <td>343223</td>\n",
       "      <td>B000WFEN74</td>\n",
       "      <td>A14PLPD8AWSF3R</td>\n",
       "      <td>A. isa \"sevendaymagic\"</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1340928000</td>\n",
       "      <td>Most of the Cans Were Dented</td>\n",
       "      <td>I've been ordering a lot of these. Many of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>163165</td>\n",
       "      <td>163166</td>\n",
       "      <td>B00015UC8O</td>\n",
       "      <td>AR6KPQ78PW5WG</td>\n",
       "      <td>jturney</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1285027200</td>\n",
       "      <td>baking aide</td>\n",
       "      <td>I'm learning to make sour dough bread, and the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>98713</td>\n",
       "      <td>98714</td>\n",
       "      <td>B001EO5Q1E</td>\n",
       "      <td>A2FPN31EGMHSIB</td>\n",
       "      <td>LinLou</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1293753600</td>\n",
       "      <td>pleased</td>\n",
       "      <td>I cannot find this product in any store,so I w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>550051</td>\n",
       "      <td>550052</td>\n",
       "      <td>B000CQIDJM</td>\n",
       "      <td>A12E78L9515IF6</td>\n",
       "      <td>Husam J. Alnajjar</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1209686400</td>\n",
       "      <td>only a hint of flavour</td>\n",
       "      <td>Been a fan of Stash teas, but I never ventured...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      Id   ProductId          UserId               ProfileName  \\\n",
       "0        1395    1396  B00068PCTU  A1WLOJBYGZ49HL  Brian F., Gloucester, MA   \n",
       "1      343222  343223  B000WFEN74  A14PLPD8AWSF3R    A. isa \"sevendaymagic\"   \n",
       "2      163165  163166  B00015UC8O   AR6KPQ78PW5WG                   jturney   \n",
       "3       98713   98714  B001EO5Q1E  A2FPN31EGMHSIB                    LinLou   \n",
       "4      550051  550052  B000CQIDJM  A12E78L9515IF6         Husam J. Alnajjar   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     0                       0      5  1333843200   \n",
       "1                     3                       5      2  1340928000   \n",
       "2                     4                       4      5  1285027200   \n",
       "3                     0                       0      5  1293753600   \n",
       "4                     0                       0      4  1209686400   \n",
       "\n",
       "                        Summary  \\\n",
       "0       The best I've ever had!   \n",
       "1  Most of the Cans Were Dented   \n",
       "2                   baking aide   \n",
       "3                       pleased   \n",
       "4        only a hint of flavour   \n",
       "\n",
       "                                                Text  \n",
       "0  I am so happy I discovered this product. I had...  \n",
       "1  I've been ordering a lot of these. Many of the...  \n",
       "2  I'm learning to make sour dough bread, and the...  \n",
       "3  I cannot find this product in any store,so I w...  \n",
       "4  Been a fan of Stash teas, but I never ventured...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df = pd.DataFrame(pd.read_csv(\"Review_word2vec.csv\"))\n",
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I cannot find this product in any store,so I was very pleased to be able to get it at Amazon.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_text = list(reviews_df.Text)\n",
    "reviews_text[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec:\n",
    "    \n",
    "    \"\"\"\n",
    "    Initializes global constants:\n",
    "        corpus\n",
    "        embedding_size\n",
    "        training epochs\n",
    "        window size\n",
    "        vocab size\n",
    "    \"\"\"\n",
    "    def __init__(self, corpus, window_size):\n",
    "        self.corpus = corpus \n",
    "        self.embedding_size = 300\n",
    "        self.epochs = 10\n",
    "        self.window_size = window_size\n",
    "        self.vocab_size = None\n",
    "    \n",
    "    \"\"\"\n",
    "    Removes stop words and puncuation from all reviews\n",
    "    \n",
    "    return: clean_docs\n",
    "    \"\"\"\n",
    "    def _remove_stop(self):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        punctuation = set(string.punctuation)\n",
    "        \n",
    "        clean_docs = []\n",
    "        \n",
    "        for doc in self.corpus:\n",
    "            # Remove stops\n",
    "            no_stops = ' '.join([word for word in doc.lower().split() if word not in stop_words])\n",
    "            \n",
    "            # Remove punctuation\n",
    "            punc_free = ''.join(symbol for symbol in no_stops if symbol not in punctuation)\n",
    "            clean_docs.append(punc_free)\n",
    "                    \n",
    "        return clean_docs\n",
    "    \n",
    "    \"\"\"\n",
    "    The preprocessing function removes stop words, lemmatizes, and reduces\n",
    "    the corpus size based on if a review contains 'coffee', 'pasta', 'tuna',\n",
    "    or 'cookies'. A subset of these reviews are used for the final corpus.\n",
    "    This function also performs the sliding window function and creates one hot vectors.\n",
    "    \n",
    "    return: input_words\n",
    "            target_words\n",
    "            one_hot_encodings_x\n",
    "            one_hot_encodings_y\n",
    "            vocab_size\n",
    "    \"\"\"\n",
    "    def _preprocess(self):\n",
    "        \n",
    "        print('Removing Stop Words...', '\\n')\n",
    "        \n",
    "        no_stops = self._remove_stop()\n",
    "\n",
    "        print('Lemmatizing...', '\\n')\n",
    "        \n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        parts_of_speech = ['ADJ','NOUN','VERB']\n",
    "        \n",
    "        #temp_corpus = []\n",
    "        #for review in no_stops:\n",
    "            #doc = nlp(review)\n",
    "            #sentences = list(doc.sents)\n",
    "        \n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        temp_corpus = []\n",
    "        \n",
    "        for review in no_stops:\n",
    "            doc = nlp(review)\n",
    "            lemmatized = \" \".join(lemmatizer.lemmatize(word) for word in doc.split() \n",
    "                                  if word.pos_ in parts_of_speech)\n",
    "            lemmatized = [value for value in lemmatized.split() if not value.isdigit()]\n",
    "            temp_corpus.append(lemmatized)\n",
    "        \n",
    "        final_corpus = []\n",
    "        \n",
    "        for review in temp_corpus:\n",
    "            if 'coffee' in review:\n",
    "                final_corpus.append(review)\n",
    "            elif 'pasta' in review:\n",
    "                final_corpus.append(review)\n",
    "            elif 'tuna' in review:\n",
    "                final_corpus.append(review)\n",
    "            elif 'cookies' in review:\n",
    "                final_corpus.append(review)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        final_corpus = random.sample(final_corpus, int((len(final_corpus)/5)))\n",
    "        set_of_words = set([word for sentence in final_corpus for word in sentence])\n",
    "    \n",
    "        vocab_size = len(set_of_words)\n",
    "            \n",
    "        print('Utilizing sliding window to find word groupings...', '\\n')\n",
    "        neighbors = self._window_slider(final_corpus)\n",
    "        \n",
    "        input_words = [i[0] for i in neighbors]\n",
    "        target_words = [i[1] for i in neighbors]\n",
    "        \n",
    "        print('Creating one hot encodings...', '\\n')\n",
    "\n",
    "        one_hot_encodings_x = self._one_hot(input_words, vocab_size)\n",
    "        one_hot_encodings_y = self._one_hot(target_words, vocab_size)\n",
    "        \n",
    "        print('Preprocessing Finished!', '\\n')\n",
    "        \n",
    "        return input_words, target_words, one_hot_encodings_x, one_hot_encodings_y, vocab_size\n",
    "    \n",
    "    \"\"\"\n",
    "    Performs a sliding window mechanism that which selects words within a specified length.\n",
    "    The window moves over one index each iteration.\n",
    "    \n",
    "    return: neighbors - pairs of words\n",
    "    \"\"\"\n",
    "    def _window_slider(self, corpus):\n",
    "        neighbors = []\n",
    "        for sentence in corpus:\n",
    "            # Specify window\n",
    "            window = [sentence[x:x+self.window_size] for x in range(len(sentence) - self.window_size + 1 )]\n",
    "            \n",
    "            # Get context and center words\n",
    "            for triplet in window:\n",
    "                center_index = int((len(triplet) - 1) / 2)\n",
    "                context = sum([triplet[0:center_index],triplet[center_index+1:len(triplet)]], [])\n",
    "                \n",
    "                # Create pairs\n",
    "                for context_word in context:\n",
    "                    pair = [triplet[center_index], context_word]\n",
    "                    neighbors.append(pair)\n",
    "\n",
    "        return neighbors\n",
    "    \n",
    "    \"\"\"\n",
    "    Creates one hot encodings for input and target words. Using Tensorflow's \n",
    "    one hot encoding function so that the model can understand the inputs.\n",
    "    \n",
    "    The one hot encoding array returns indices for each word. Each index represents\n",
    "    the location of \"1\"\n",
    "    \n",
    "    return: one_hot_encodings\n",
    "    \"\"\"\n",
    "    def _one_hot(self, words, vocab_size):\n",
    "        one_hot_encodings = [one_hot(word, vocab_size) for word in words]\n",
    "        return one_hot_encodings\n",
    "    \n",
    "    \"\"\"\n",
    "    Run the tensorflow sequential model with custom backend loss function.\n",
    "    \n",
    "    return: embedding layer weights\n",
    "    \"\"\"\n",
    "    def _run_model(self, x, y, vocab_size):\n",
    "        \n",
    "        print('Run Model...', '\\n')\n",
    "        \n",
    "        model = Sequential()\n",
    "        \n",
    "        # Create embedding layer: vocab_size x self.embedding_size\n",
    "        model.add(Embedding(vocab_size, self.embedding_size, \n",
    "                            input_length=1, name=\"embedding\"))\n",
    "        model.add(Flatten())\n",
    "        \n",
    "        # Output layer with softmax activation\n",
    "        model.add(Dense(1, activation='softmax'))\n",
    "        \n",
    "        # Subgradient Descent optimizer\n",
    "        optimizer = tf.keras.optimizers.SGD()\n",
    "        \n",
    "        # Use custom loss function instead of built-in function\n",
    "        model.compile(optimizer=optimizer, loss=self._cross_entropy_loss, metrics=['accuracy'])\n",
    "        \n",
    "        # Fit the model\n",
    "        model.fit(x, y, epochs=10, verbose=0)\n",
    "        \n",
    "        print('Training Finished!')\n",
    "        \n",
    "        weights = model.get_layer('embedding').get_weights()[0]\n",
    "        \n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    \"\"\"\n",
    "    Computes Cross Entropy Loss for Skip-gram network\n",
    "    \n",
    "    return: loss\n",
    "    \"\"\"\n",
    "    def _cross_entropy_loss(self, actual, pred):\n",
    "        actual = tf.cast(actual, tf.float32)\n",
    "        pred = tf.cast(pred, tf.float32)\n",
    "        error = actual*pred\n",
    "        loss = -K.sum(error)\n",
    "        return loss\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns the cosine similarity between two vectors\n",
    "    \"\"\"\n",
    "    def _cosine_similarity(self, word_vec_1, word_vec_2):\n",
    "        return (1 - spatial.distance.cosine(word_vec_1, word_vec_2))\n",
    "    \n",
    "    \"\"\"\n",
    "    Solves Analogies given 3 input words, and the word_vectors\n",
    "    Example: Spain is to Spanish as Germany is to German\n",
    "    return: missing_word - word with highest cosine similarity\n",
    "    \"\"\"\n",
    "    def _solve_analogy(self, word_1, word_2, word_3, word_vectors):\n",
    "        # Initialize as -1 at beginning of iteration \n",
    "        # because values will be higher than this one\n",
    "        best_similarity = -1\n",
    "\n",
    "        missing_word = None\n",
    "        \n",
    "        # Word vectors for input words\n",
    "        vec_1,vec_2,vec_3 = np.array(word_vectors[f'{word_1}']).astype(float), \\\n",
    "                               np.array(word_vectors[f'{word_2}']).astype(float), \\\n",
    "                               np.array(word_vectors[f'{word_3}']).astype(float)\n",
    "        \n",
    "        # Only compute cosine similarity for all word vecs\n",
    "        # excluding the input vectos\n",
    "        for i in word_vectors.columns:\n",
    "            if i in [word_1,word_2,word_3]:\n",
    "                continue\n",
    "            \n",
    "            # Test vector\n",
    "            try_vector = np.array(word_vectors[f'{i}']).astype(float)\n",
    "            \n",
    "            # Compute cosine similarity\n",
    "            # Logic is as follows: Spanish - Spain (Country) = try_vector - German\n",
    "            similarity = cosine_similarity([vec_2-vec_1], [try_vector-vec_3])\n",
    "            \n",
    "            if similarity > best_similarity:\n",
    "                best_similarity = similarity\n",
    "                missing_word = i     \n",
    "\n",
    "        return missing_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing Stop Words... \n",
      "\n",
      "Lemmatizing... \n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'spacy.tokens.doc.Doc' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-656c171adb98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minput_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-be05a1e65745>\u001b[0m in \u001b[0;36m_preprocess\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mno_stops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             lemmatized = \" \".join(lemmatizer.lemmatize(word) for word in doc.split() \n\u001b[0m\u001b[1;32m     73\u001b[0m                                   if word.pos_ in parts_of_speech)\n\u001b[1;32m     74\u001b[0m             \u001b[0mlemmatized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlemmatized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'spacy.tokens.doc.Doc' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(reviews_text, window_size = 5)\n",
    "input_words, target_words, x, y, vocab_size = model._preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model._run_model(x, y, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Find Similar Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['coffee', 'pasta', 'tuna', 'cookie']\n",
    "\n",
    "for word in words:\n",
    "    sims = []\n",
    "    indices = []\n",
    "    for index, embedding in enumerate(embeddings):\n",
    "\n",
    "        if x[input_words.index(word)][0] == index:\n",
    "            continue\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        similarity = model._cosine_similarity(embeddings[x[input_words.index(word)][0]].reshape(-1, 1), \n",
    "                                              embeddings[index].reshape(-1, 1))\n",
    "        sims.append(similarity)\n",
    "        indices.append(index)\n",
    "\n",
    "    sims = np.array(sims)\n",
    "    indices = np.array(indices)\n",
    "    \n",
    "    # Get top 10 words\n",
    "    top_words = [target_words[i] for i in list(sims.argsort()[-10:])]\n",
    "    print(f'The top ten words for {word} are: {top_words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word Analogies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_list = []\n",
    "with open('new_glove.txt') as file:\n",
    "    for line in file:\n",
    "        inner_list = [word.rstrip() for word in line.split(' ')]\n",
    "        glove_list.append(inner_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dict = {}\n",
    "\n",
    "for embedding in glove_list:\n",
    "    glove_dict[f'{embedding[0]}'] = embedding[1:len(embedding)]\n",
    "    \n",
    "glove_df = pd.DataFrame(glove_dict)\n",
    "glove_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogy_1 = model._solve_analogy('spain', 'spanish', 'germany', glove_df)\n",
    "analogy_2 = model._solve_analogy('japan', 'tokyo', 'france', glove_df)\n",
    "analogy_3 = model._solve_analogy('woman', 'man', 'queen', glove_df)\n",
    "analogy_4 = model._solve_analogy('australia', 'hotdog', 'italy', glove_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Spain is to Spanish as Germany is to {str(analogy_1)}')\n",
    "print(f'Japan is to Tokyo as France is to {str(analogy_2)}')\n",
    "print(f'Woman is to Man as Queen is to {str(analogy_3)}')\n",
    "print(f'Australia is to Hotdog as Italy is to {str(analogy_4)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
